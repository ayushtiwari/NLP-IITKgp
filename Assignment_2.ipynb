{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zyJ25uz0kSaw"
      },
      "source": [
        "# Assignment 2 on Natural Language Processing\n",
        "\n",
        "### Date : 15th Sept, 2020\n",
        "\n",
        "### Instructor : Prof. Sudeshna Sarkar\n",
        "\n",
        "### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Anusha Potnuru, Uppada Vishnu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ao1nhg9RknmF"
      },
      "source": [
        "The central idea of this assignment is to make you familiar with programming in python and also the language modelling task of natural language processing using the python.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "stk58juYkzEr"
      },
      "source": [
        "**Dataset:** \n",
        "\n",
        " Use the text file provided along."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rT6byv49kdmo",
        "colab": {}
      },
      "source": [
        "# read dataset\n",
        "text = open('corpus.txt', 'r').read()\n",
        "# print(text)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SRGqKaDn1pJy"
      },
      "source": [
        "Preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C1OtHn6B1oc2",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "text_p = []\n",
        "\n",
        "for sentence in sentences:\n",
        "  sentence = sentence.lower()\n",
        "  sentence = re.sub(r'[^A-Za-z ]', '', sentence)\n",
        "  words = sentence.split(' ')\n",
        "  text_p.append(words)\n",
        "\n",
        "# text_p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YDL7yfpXkMRS"
      },
      "source": [
        "### Task: In this sub-task, you are expected to carry out the following tasks:\n",
        "\n",
        "1. **Create the following language models** on the training corpus: <br>\n",
        "    i.   Unigram <br>\n",
        "    ii.  Bigram <br>\n",
        "    iii. Trigram <br>\n",
        "    iv.  Fourgram <br>\n",
        "\n",
        "2. **List the top 5 bigrams, trigrams, four-grams (with and without Add-1 smoothing).**\n",
        "(Note: Please remove those which contain only articles, prepositions, determiners. For Example: “of the”, “in a”, etc)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u3oIulBikPua",
        "colab": {}
      },
      "source": [
        "#Write code\n",
        "\n",
        "from nltk.util import ngrams\n",
        "unigrams=[]\n",
        "bigrams=[]\n",
        "trigrams=[]\n",
        "fourgrams=[]\n",
        "\n",
        "for content in text_p:\n",
        "    unigrams.extend(content)\n",
        "    bigrams.extend(ngrams(content,2))\n",
        "    trigrams.extend(ngrams(content, 3))\n",
        "    fourgrams.extend(ngrams(content, 4))\n",
        "    ##similar for trigrams and fourgrams\n",
        "\n",
        "print(bigrams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vARsvSfynePr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "1ab27126-48ff-47ba-db81-6af05b2f445b"
      },
      "source": [
        "#stopwords = code for downloading stop words through nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stops = stopwords.words('english')\n",
        "\n",
        "def get_clean_grams(grams):\n",
        "  clean = [gram for gram in grams if not any(stop in gram for stop in stops)]\n",
        "  return clean\n",
        "\n",
        "#print top 10 unigrams, bigrams after removing stopwords\n",
        "uni_processed = [p for p in unigrams if p not in stops]\n",
        "fdist = nltk.FreqDist(uni_processed)\n",
        "print(fdist.most_common(10))\n",
        "\n",
        "bi_processed = [gram for gram in bigrams if not set(gram) <= set(stops)]\n",
        "fdist = nltk.FreqDist(bi_processed)\n",
        "print(fdist.most_common(10))\n",
        "\n",
        "tri_processed = [gram for gram in trigrams if not set(gram) <= set(stops)]\n",
        "fdist = nltk.FreqDist(tri_processed)\n",
        "print(fdist.most_common(10))\n",
        "\n",
        "four_processed = [gram for gram in fourgrams if not set(gram) <= set(stops)]\n",
        "fdist = nltk.FreqDist(four_processed)\n",
        "print(fdist.most_common(10))\n",
        "#print top 10 bigrams, trigrams, fourgrams after removing stopwords\n",
        "\n"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[('said', 462), ('alice', 385), ('little', 128), ('one', 101), ('like', 85), ('know', 85), ('would', 83), ('went', 83), ('could', 77), ('thought', 74)]\n",
            "[(('said', 'the'), 209), (('said', 'alice'), 115), (('the', 'queen'), 65), (('the', 'king'), 60), (('a', 'little'), 59), (('mock', 'turtle'), 54), (('the', 'mock'), 53), (('the', 'gryphon'), 53), (('the', 'hatter'), 51), (('went', 'on'), 48)]\n",
            "[(('the', 'mock', 'turtle'), 51), (('the', 'march', 'hare'), 30), (('said', 'the', 'king'), 29), (('the', 'white', 'rabbit'), 21), (('said', 'the', 'hatter'), 21), (('said', 'to', 'herself'), 19), (('said', 'the', 'mock'), 19), (('said', 'the', 'caterpillar'), 18), (('she', 'went', 'on'), 17), (('she', 'said', 'to'), 17)]\n",
            "[(('said', 'the', 'mock', 'turtle'), 19), (('she', 'said', 'to', 'herself'), 16), (('a', 'minute', 'or', 'two'), 11), (('said', 'the', 'march', 'hare'), 8), (('will', 'you', 'wont', 'you'), 8), (('said', 'alice', 'in', 'a'), 7), (('as', 'well', 'as', 'she'), 6), (('well', 'as', 'she', 'could'), 6), (('in', 'a', 'great', 'hurry'), 6), (('in', 'a', 'tone', 'of'), 6)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ioc1xNjmnim-"
      },
      "source": [
        "# Applying Smoothing\n",
        "\n",
        "\n",
        "Assume additional training data in which each possible N-gram occurs exactly once and adjust estimates.\n",
        "\n",
        "> ### $ Probability(ngram) = \\frac{Count(ngram)+1}{ N\\, +\\, V} $\n",
        "\n",
        "N: Total number of N-grams <br>\n",
        "V: Number of unique N-grams\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "grh4sO0Yns4V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "fd4f9c8a-0d32-422c-f56c-60081beb8d1b"
      },
      "source": [
        "#You are to perform Add-1 smoothing here:\n",
        "#write similar code for bigram, trigram and fourgrams\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "def smoothed(grams):\n",
        "  res = dict()\n",
        "\n",
        "  N = len(grams)\n",
        "  V = len(set(grams))\n",
        "\n",
        "  print(\"N = %d, V = %d\" % (N, V))\n",
        "\n",
        "  counter = Counter(grams)\n",
        "\n",
        "  for gram in grams:\n",
        "    res[gram] = (counter[gram] + 1)/(N+V)\n",
        "\n",
        "  return res\n",
        "\n",
        "\n",
        "\n",
        "import heapq\n",
        "\n",
        "uni_smoothed = smoothed(uni_processed)\n",
        "print(heapq.nlargest(10, uni_smoothed, key=uni_smoothed.get))\n",
        "\n",
        "bi_smoothed = smoothed(bi_processed)\n",
        "print(heapq.nlargest(10, bi_smoothed, key=bi_smoothed.get))\n",
        "\n",
        "tri_smoothed = smoothed(tri_processed)\n",
        "print(heapq.nlargest(10, tri_smoothed, key=tri_smoothed.get))\n",
        "\n",
        "four_smoothed = smoothed(four_processed)\n",
        "print(heapq.nlargest(10, four_smoothed, key=four_smoothed.get))\n",
        "#Print top 10 unigram, bigram, trigram, fourgram after smoothing\n"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "N = 12620, V = 2623\n",
            "['said', 'alice', 'little', 'one', 'like', 'know', 'would', 'went', 'could', 'thought']\n",
            "N = 19522, V = 12309\n",
            "[('said', 'the'), ('said', 'alice'), ('the', 'queen'), ('the', 'king'), ('a', 'little'), ('mock', 'turtle'), ('the', 'mock'), ('the', 'gryphon'), ('the', 'hatter'), ('went', 'on')]\n",
            "N = 22328, V = 19418\n",
            "[('the', 'mock', 'turtle'), ('the', 'march', 'hare'), ('said', 'the', 'king'), ('the', 'white', 'rabbit'), ('said', 'the', 'hatter'), ('said', 'to', 'herself'), ('said', 'the', 'mock'), ('said', 'the', 'caterpillar'), ('she', 'went', 'on'), ('she', 'said', 'to')]\n",
            "N = 22746, V = 21830\n",
            "[('said', 'the', 'mock', 'turtle'), ('she', 'said', 'to', 'herself'), ('a', 'minute', 'or', 'two'), ('said', 'the', 'march', 'hare'), ('will', 'you', 'wont', 'you'), ('said', 'alice', 'in', 'a'), ('as', 'well', 'as', 'she'), ('well', 'as', 'she', 'could'), ('in', 'a', 'great', 'hurry'), ('in', 'a', 'tone', 'of')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k0GL40mQmmt4"
      },
      "source": [
        "### Predict the next word using statistical language modelling\n",
        "\n",
        "Using the above bigram, trigram, and fourgram models that you just experimented with, **predict the next word(top 5 probable) given the previous n(=2, 3, 4)-grams** for the sentences below.\n",
        "\n",
        "For str1, str2, you are to predict the next  2 possible word sequences using your trained smoothed models. <br> \n",
        "For example, for the string 'He looked very' the answers can be as below: \n",
        ">     (1) 'He looked very' *anxiouxly* \n",
        ">     (2) 'He looked very' *uncomfortable* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MBWKo5_Fmnbg",
        "colab": {}
      },
      "source": [
        "str1 = 'after that alice said the'\n",
        "str2 = 'alice felt so desperate that she was'"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ext_nVn2mvZt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "outputId": "2c6cecac-cdd3-4a9e-e236-c2a53dbf6496"
      },
      "source": [
        "#write code\n",
        "\n",
        "word_not_to_consider = []\n",
        "\n",
        "def predict_next_word_bigram(sentence):\n",
        "  words = sentence.split(' ')\n",
        "  word_z = words[-1]\n",
        "\n",
        "  max = 0\n",
        "  candidate = None\n",
        "  for key, value in bi_smoothed.items():\n",
        "    if (key[0]==word_z):\n",
        "      if (value > max and key[1] not in word_not_to_consider):\n",
        "        candidate = key[1]\n",
        "        max = value\n",
        "  return candidate\n",
        "\n",
        "\n",
        "def predict_next_word_trigram(sentence):\n",
        "  words = sentence.split(' ')\n",
        "  word_z = words[-1]\n",
        "  word_y = words[-2]\n",
        "\n",
        "  max = 0\n",
        "  candidate = None\n",
        "  for key, value in tri_smoothed.items():\n",
        "    if (key[0]==word_y and key[1]==word_z):\n",
        "      if (value > max and key[2] not in word_not_to_consider):\n",
        "        candidate = key[2]\n",
        "        max = value\n",
        "  return candidate\n",
        "\n",
        "def predict_next_word_fourgram(sentence):\n",
        "  words = sentence.split(' ')\n",
        "  word_z = words[-1]\n",
        "  word_y = words[-2]\n",
        "  word_x = words[-3]\n",
        "\n",
        "  max = 0\n",
        "  candidate = None\n",
        "  for key, value in four_smoothed.items():\n",
        "    if (key[0]==word_x and key[1]==word_y and key[2]==word_z):\n",
        "      if (value > max and key[3] not in word_not_to_consider):\n",
        "        candidate = key[3]\n",
        "        max = value\n",
        "  return candidate\n",
        "\n",
        "print(\"---Bigrams---\")\n",
        "for s in (str1, str2):\n",
        "  print(\"\\n%s ______\" % s)\n",
        "  for i in range(5):\n",
        "    next_word = predict_next_word_bigram(s)\n",
        "    print(next_word)\n",
        "    word_not_to_consider.append(next_word)\n",
        "\n",
        "print(\"\")\n",
        "word_not_to_consider = []\n",
        "\n",
        "print(\"---Trigrams---\")\n",
        "for s in (str1, str2):\n",
        "  print(\"\\n%s ______\" % s)\n",
        "  for i in range(5):\n",
        "    next_word = predict_next_word_trigram(s)\n",
        "    print(next_word)\n",
        "    word_not_to_consider.append(next_word)\n",
        "\n",
        "print(\"\")\n",
        "word_not_to_consider = []\n",
        "\n",
        "print(\"---Fourgrams---\")\n",
        "for s in (str1, str2):\n",
        "  print(\"\\n%s ______\" % s)\n",
        "  for i in range(5):\n",
        "    next_word = predict_next_word_fourgram(s)\n",
        "    print(next_word)\n",
        "    word_not_to_consider.append(next_word)\n",
        "\n",
        "# print(predict_next_word_trigram(\"he looked very\"))\n",
        "# print(predict_next_word_fourgram(\"he looked very\"))"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---Bigrams---\n",
            "\n",
            "after that alice said the ______\n",
            "queen\n",
            "king\n",
            "mock\n",
            "gryphon\n",
            "hatter\n",
            "\n",
            "alice felt so desperate that she was ______\n",
            "going\n",
            "quite\n",
            "looking\n",
            "sitting\n",
            "beginning\n",
            "\n",
            "---Trigrams---\n",
            "\n",
            "after that alice said the ______\n",
            "king\n",
            "hatter\n",
            "mock\n",
            "caterpillar\n",
            "gryphon\n",
            "\n",
            "alice felt so desperate that she was ______\n",
            "quite\n",
            "walking\n",
            "looking\n",
            "beginning\n",
            "considering\n",
            "\n",
            "---Fourgrams---\n",
            "\n",
            "after that alice said the ______\n",
            "None\n",
            "None\n",
            "None\n",
            "None\n",
            "None\n",
            "\n",
            "alice felt so desperate that she was ______\n",
            "dozing\n",
            "walking\n",
            "ready\n",
            "losing\n",
            "quite\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}